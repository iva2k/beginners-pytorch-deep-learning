{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 4: Transfer Learning And Other Tricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.utils.data\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "import torchvision.models as models\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Settings\n",
    "images_path = '../images'\n",
    "epochs = 10\n",
    "# model_filename=\"/tmp/resnet50\"\n",
    "model_filename=\"./saved/resnet50\"\n",
    "lr=1e-3 # Note: Overriden below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Init run\n",
    "first_run = False if \"first_run\" in globals() else True\n",
    "print(f\"First run: {first_run}\")\n",
    "if first_run:\n",
    "    # Reset vars\n",
    "    total_epochs = 0\n",
    "    total_model_params = 0\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Misc functions\n",
    "\n",
    "def count_parameters(model: nn.Module) -> int:\n",
    "    return sum(p.numel() for p in model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Model\n",
    "if first_run:\n",
    "    transfer_model = models.resnet50(pretrained=True)\n",
    "    total_model_params = count_parameters(transfer_model)\n",
    "    print(f'{total_model_params} parameters')\n",
    "    print(transfer_model)\n",
    "else:\n",
    "    print(\"Skipped after first run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Freezing parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze all params (skip on 1st run)\n",
    "if not first_run:\n",
    "    for name, param in transfer_model.named_parameters():\n",
    "        param.requires_grad = True\n",
    "else:\n",
    "    print(\"Skipped on first run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfreeze select layers (skip on 1st run)\n",
    "if not first_run:\n",
    "    unfreeze_layers = [transfer_model.layer3, transfer_model.layer4]\n",
    "    count_unfrozen = 0\n",
    "    for layer in unfreeze_layers:\n",
    "        for param in layer.parameters():\n",
    "            count_unfrozen += 1\n",
    "            param.requires_grad = True\n",
    "    print(f'Unfrozen {count_unfrozen} parameters')\n",
    "else:\n",
    "    print(\"Skipped on first run.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze named params (only in not \"*bn*\" layers)\n",
    "count_frozen = 0\n",
    "count_all = 0\n",
    "for name, param in transfer_model.named_parameters():\n",
    "    count_all += 1\n",
    "    if(\"bn\" not in name):\n",
    "        count_frozen += 1\n",
    "        param.requires_grad = False\n",
    "print(f'Frozen {count_frozen} of total {count_all} named parameters ({count_frozen/count_all*100:.5f}%)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Replacing the classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if first_run:\n",
    "    transfer_model.fc = nn.Sequential(nn.Linear(transfer_model.fc.in_features,500),\n",
    "        nn.ReLU(),                                 \n",
    "        nn.Dropout(), nn.Linear(500,2))\n",
    "else:\n",
    "    print(\"Skipped after first run.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transforms\n",
    "\n",
    "Here we'll create a lambda transform and a custom transform class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom Transforms\n",
    "\n",
    "# Convert RGB to HSV color space\n",
    "def _random_colour_space(x):\n",
    "    output = x.convert(\"HSV\")\n",
    "    return output\n",
    "colour_transform = transforms.Lambda(lambda x: _random_colour_space(x))\n",
    "random_colour_transform = transforms.RandomApply([colour_transform])\n",
    "\n",
    "class GaussNoise():\n",
    "    \"\"\"Adds gaussian noise to a tensor.\n",
    "    \n",
    "    Example:\n",
    "        >>> transforms.Compose([\n",
    "        >>>     transforms.ToTensor(),\n",
    "        >>>     Noise(0.1, 0.05)),\n",
    "        >>> ])\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, mean, stddev):\n",
    "        self.mean = mean\n",
    "        self.stddev = stddev\n",
    "\n",
    "    def __call__(self, tensor):\n",
    "        noise = torch.zeros_like(tensor).normal_(self.mean, self.stddev)\n",
    "        return tensor.add_(noise)\n",
    "    \n",
    "    def __repr__(self):\n",
    "        repr = f\"{self.__class__.__name__  }(mean={self.mean},sttdev={self.stddev})\"\n",
    "        return repr\n",
    "    \n",
    "# custom_transform_pipeline = transforms.Compose([random_colour_transform, transforms.ToTensor(), GaussNoise(0.1, 0.05)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training Data\n",
    "def check_image(path):\n",
    "    try:\n",
    "        _im = Image.open(path)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'Invalid image file \"{path}\", error {e}')\n",
    "        return False\n",
    "\n",
    "transforms_list = []\n",
    "if not first_run:\n",
    "    transforms_list += [\n",
    "        # Data Augmentation (PIL Image space)\n",
    "        random_colour_transform,\n",
    "    ]\n",
    "transforms_list += [\n",
    "    transforms.Resize((64,64)),    \n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
    "                         std=[0.229, 0.224, 0.225] ),\n",
    "\n",
    "    # Data Augmentation:\n",
    "    # GaussNoise(0.1, 0.05),\n",
    "    # transforms.RandomRotation(degrees=15, interpolation=transforms.InterpolationMode.NEAREST, expand=False, center=None),\n",
    "    # transforms.RandomHorizontalFlip(p=0.5),\n",
    "    # transforms.RandomVerticalFlip(p=0.5),\n",
    "    # transforms.RandomGrayscale(p=0.1),\n",
    "]\n",
    "if not first_run:\n",
    "    transforms_list += [\n",
    "        # Data Augmentation:\n",
    "        # GaussNoise(0.1, 0.05),\n",
    "        # transforms.RandomRotation(degrees=15, interpolation=transforms.InterpolationMode.NEAREST, expand=False, center=None),\n",
    "        transforms.RandomHorizontalFlip(p=0.5),\n",
    "        transforms.RandomVerticalFlip(p=0.5),\n",
    "        # transforms.RandomGrayscale(p=0.1),\n",
    "    ]\n",
    "img_transforms = transforms.Compose(transforms_list)\n",
    "train_data_path = os.path.join(images_path, \"train\")\n",
    "test_data_path = os.path.join(images_path, \"test\")\n",
    "val_data_path = os.path.join(images_path, \"val\")\n",
    "train_data = torchvision.datasets.ImageFolder(root=train_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "test_data = torchvision.datasets.ImageFolder(root=test_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "val_data = torchvision.datasets.ImageFolder(root=val_data_path,transform=img_transforms, is_valid_file=check_image)\n",
    "batch_size=64\n",
    "train_data_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_data_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True)\n",
    "val_data_loader  = torch.utils.data.DataLoader(val_data, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "print(f'Training data length: {len(train_data_loader.dataset)}')\n",
    "print(f'Test data length: {len(test_data_loader.dataset)}') # UNUSED!\n",
    "print(f'Validation data length: {len(val_data_loader.dataset)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, optimizer, loss_fn, train_loader, val_loader, epochs=20, device=\"cpu\"):\n",
    "    for epoch in range(1, epochs+1):\n",
    "        training_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        model.train()\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            targets = targets.to(device)\n",
    "            output = model(inputs)\n",
    "            loss = loss_fn(output, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            training_loss += loss.data.item() * inputs.size(0)\n",
    "        training_loss /= len(train_loader.dataset)\n",
    "        \n",
    "        model.eval()\n",
    "        num_correct = 0 \n",
    "        num_examples = 0\n",
    "        for batch in val_loader:\n",
    "            inputs, targets = batch\n",
    "            inputs = inputs.to(device)\n",
    "            output = model(inputs)\n",
    "            targets = targets.to(device)\n",
    "            loss = loss_fn(output,targets) \n",
    "            valid_loss += loss.data.item() * inputs.size(0)\n",
    "            correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
    "            num_correct += torch.sum(correct).item()\n",
    "            num_examples += correct.shape[0]\n",
    "        valid_loss /= len(val_loader.dataset)\n",
    "\n",
    "        print('Epoch: {}, Training Loss: {:.3f}, Validation Loss: {:.3f}, accuracy = {:.3f}'.format(epoch, training_loss,\n",
    "        valid_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select CUDA vs CPU\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(\"Using CUDA\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "    print(\"Using CPU\")\n",
    "\n",
    "transfer_model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adjust LR\n",
    "lr = 1e-3\n",
    "if not first_run:\n",
    "    lr = 1e-6\n",
    "    # lr = 2e-7\n",
    "print(f\"Set Learn Rate lr={lr}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer1 = optim.Adam(transfer_model.parameters(), lr=lr)\n",
    "\n",
    "optimizer2 = optim.Adam([\n",
    "{ 'params': transfer_model.layer4.parameters(), 'lr': lr /3},\n",
    "{ 'params': transfer_model.layer3.parameters(), 'lr': lr /9},\n",
    "], lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optimizer1 if first_run else optimizer2\n",
    "train(transfer_model, optimizer, torch.nn.CrossEntropyLoss(), train_data_loader, val_data_loader, epochs=epochs, device=device)\n",
    "total_epochs += epochs\n",
    "print(f'DONE {epochs} epochs, total epochs: {total_epochs}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(model, loss_fn, test_loader, device=\"cpu\"):\n",
    "    test_loss = 0.0\n",
    "    model.eval()\n",
    "    num_correct = 0 \n",
    "    num_examples = 0\n",
    "    for batch in test_loader:\n",
    "        inputs, targets = batch\n",
    "        inputs = inputs.to(device)\n",
    "        output = model(inputs)\n",
    "        targets = targets.to(device)\n",
    "        loss = loss_fn(output,targets) \n",
    "        test_loss += loss.data.item() * inputs.size(0)\n",
    "        correct = torch.eq(torch.max(F.softmax(output), dim=1)[1], targets).view(-1)\n",
    "        num_correct += torch.sum(correct).item()\n",
    "        num_examples += correct.shape[0]\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    print('Test Loss: {:.3f}, accuracy = {:.3f}'.format(test_loss, num_correct / num_examples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(transfer_model, torch.nn.CrossEntropyLoss(), test_data_loader, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Run Here\n",
    "first_run = False\n",
    "assert False, \"Stopping the Run. Nothing to auto-run below\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LR Finder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_lr(model, loss_fn, optimizer, train_loader, init_value=1e-8, final_value=10.0, device=\"cpu\"):\n",
    "    number_in_epoch = len(train_loader) - 1\n",
    "    update_step = (final_value / init_value) ** (1 / number_in_epoch)\n",
    "    lr = init_value\n",
    "    optimizer.param_groups[0][\"lr\"] = lr\n",
    "    best_loss = 0.0\n",
    "    batch_num = 0\n",
    "    losses = []\n",
    "    log_lrs = []\n",
    "    for data in train_loader:\n",
    "        batch_num += 1\n",
    "        inputs, targets = data\n",
    "        inputs = inputs.to(device)\n",
    "        targets = targets.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = loss_fn(outputs, targets)\n",
    "\n",
    "        # Crash out if loss explodes\n",
    "\n",
    "        if batch_num > 1 and loss > 4 * best_loss:\n",
    "            if(len(log_lrs) > 20):\n",
    "                return log_lrs[10:-5], losses[10:-5]\n",
    "            else:\n",
    "                return log_lrs, losses\n",
    "\n",
    "        # Record the best loss\n",
    "\n",
    "        if loss < best_loss or batch_num == 1:\n",
    "            best_loss = loss\n",
    "\n",
    "        # Store the values\n",
    "        losses.append(loss.item())\n",
    "        log_lrs.append((lr))\n",
    "\n",
    "        # Do the backward pass and optimize\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Update the lr for the next step and store\n",
    "\n",
    "        lr *= update_step\n",
    "        optimizer.param_groups[0][\"lr\"] = lr\n",
    "    if(len(log_lrs) > 20):\n",
    "        return log_lrs[10:-5], losses[10:-5]\n",
    "    else:\n",
    "        return log_lrs, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(lrs, losses) = find_lr(transfer_model, torch.nn.CrossEntropyLoss(), optimizer, train_data_loader, init_value=1e-7, final_value=1e-3, device=device)\n",
    "plt.plot(lrs, losses)\n",
    "\n",
    "plt.xscale(\"log\")\n",
    "plt.xlabel(\"Learning rate\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensembles\n",
    "\n",
    "Given a list of models, we can produce predictions for each model and then make an average to make a final prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_ensemble = [models.resnet50().to(device), models.resnet50().to(device)]\n",
    "predictions = [F.softmax(m(torch.rand(1,3,224,244).to(device))) for m in models_ensemble] \n",
    "avg_prediction = torch.stack(predictions).mean(0).argmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.stack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
